{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install pm4py\n",
    "# %pip install psutil\n",
    "# %pip install matplotlib\n",
    "# %pip install seaborn\n",
    "# %pip install holidays\n",
    "# %pip install scikit-learn\n",
    "# %pip install collection\n",
    "# %pip install scipy\n",
    "\n",
    "import pandas as pd\n",
    "import pm4py as pm\n",
    "import numpy as np\n",
    "import psutil as pu\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime\n",
    "import holidays\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "training_file_path = 'db/BPI_Challenge_2012.XE-training.csv'\n",
    "testing_file_path = 'db/BPI_Challenge_2012.XE-test.csv'\n",
    "nl_holidays = holidays.country_holidays('NL')\n",
    "holidays_keys = nl_holidays.keys()\n",
    "holiday_list = [i for i in nl_holidays.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load CSV with pandas and pm4py\n",
    "\n",
    "pm4py gives two extra columns: @@index and @@case_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_event_log(path):\n",
    "    event_log = pd.read_csv(path, sep=',')\n",
    "    event_log = pm.format_dataframe(\n",
    "        event_log, \n",
    "        case_id='case concept:name', \n",
    "        activity_key='event concept:name', \n",
    "        timestamp_key='event time:timestamp'\n",
    "    )\n",
    "    event_log = event_log.drop(\"case:concept:name\", axis = 1)\n",
    "    event_log = event_log.drop(\"concept:name\", axis = 1)\n",
    "    event_log = event_log.drop(\"time:timestamp\", axis = 1)\n",
    "    return event_log\n",
    "#event_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genernal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that finds the length of the longest trace in a list of traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_trace(traces):\n",
    "    max = 0\n",
    "    for trace in traces:\n",
    "        if trace.shape[0] > max:\n",
    "            max = trace.shape[0]\n",
    "    return max\n",
    "\n",
    "def get_all_Activities():\n",
    "    df = format_event_log(training_file_path)\n",
    "    arr = df['event concept:name'].unique()\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that adds the time difference from the last event in the trace column in the data set for time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume sorted by caseID and time\n",
    "def caseHasNextEvent(df, index):\n",
    "    if index >= len(df) - 1:\n",
    "        return False\n",
    "    if df.loc[index, 'case concept:name'] == df.loc[index+1, 'case concept:name']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def computeTimeDifference(df):\n",
    "    # df['time_until_next_event'] = 0  # initialize new column with zeros\n",
    "    df = df.assign(ground_truth_time=0) # initialize new column\n",
    "\n",
    "    # iterate over each row of the dataframe\n",
    "    for i, row in df.iterrows():        \n",
    "        # check if there is a next row with the same case\n",
    "        if caseHasNextEvent(df, i):\n",
    "            nextTime = df.loc[i+1, 'event time:timestamp']\n",
    "            currentTime = row['event time:timestamp']\n",
    "            timeDiff = nextTime - currentTime\n",
    "            df.at[i, 'ground_truth_time'] = timeDiff.total_seconds()\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to write ground truth column in the data set for verification purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def writeGroundtruth(df):\n",
    "    df = df.sort_values(by=['case concept:name','event time:timestamp'])\n",
    "    #add new columns containing the name of the next event in the case and the time when it happens\n",
    "    df = df.assign(ground_truth_activity='')\n",
    "    df = df.assign(ground_truth_time='')\n",
    "\n",
    "    for ind in df.index:\n",
    "        if caseHasNextEvent(df, ind):\n",
    "            df.at[ind,'ground_truth_activity'] = df.loc[ind+1,'event concept:name']\n",
    "            #df.at[ind,'ground_truth_time'] = df.loc[ind+1, 'event time:timestamp']\n",
    "        else:\n",
    "            df.at[ind,'ground_truth_activity'] = None\n",
    "            #df.at[ind,'ground_truth_time'] = None\n",
    "    df = computeTimeDifference(df)\n",
    "    return df\n",
    "\n",
    "#df_event = writeGroundtruth(event_log)\n",
    "#df_event.to_csv('check_writeGroundtruth_out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that splits the data set into separate traces\n",
    "- Parameter $traces$ is a list containing all the individual traces\n",
    "- Each trace in traces is a list containing all the events in the trace in the order that they happen\n",
    "- Helper method for prefix extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_traces(event_log):\n",
    "    traces = []\n",
    "    last_location = 0\n",
    "    for j in range (0, event_log.shape[0] - 1):\n",
    "        if not event_log[\"case concept:name\"][j] == event_log[\"case concept:name\"][j + 1]:\n",
    "            traces.append(event_log.loc[last_location : j].reset_index())\n",
    "            last_location = j + 1\n",
    "    traces.append(event_log.loc[last_location : event_log.shape[0] - 1].reset_index())\n",
    "    return traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that deletes traces from an event log that do not end before a specified time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_overlapping_traces(event_log, end_time):\n",
    "    traces = split_into_traces(event_log)\n",
    "    del_tr = []\n",
    "    for j in range(len(traces)):\n",
    "        for k in traces[j].index:\n",
    "            trace_ts = traces[j][\"event time:timestamp\"][k]\n",
    "        if trace_ts > end_time:\n",
    "            del_tr.append(traces[j][\"case concept:name\"][0])\n",
    "    \n",
    "    for j in range(len(del_tr)):\n",
    "        event_log = event_log[event_log[\"case concept:name\"] != del_tr[j]]\n",
    "    event_log.to_csv(\"db/S2_Remove_Overlap.csv\")\n",
    "    return event_log.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefix extraction\n",
    "- extract event lists of all lengths store in the prefix_lengts, from the traces for prediction\n",
    "- store them in a list of prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_extraction(traces, prefix_lengths):\n",
    "    prefixes = []\n",
    "    for trace in traces:\n",
    "        for length in prefix_lengths:\n",
    "            if trace.shape[0] >= length:\n",
    "                prefixes.append(trace.loc[:length - 1])\n",
    "    return prefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_independent_variable should have index as activities\n",
    "# and with a value of a discrete number.\n",
    "def process_X_Y(df, str_DV_col, dict_independent_variable):\n",
    "    independent_Variable = get_all_Activities()\n",
    "    independent_Variable = np.append(['hollidays', 'last_event'], independent_Variable)\n",
    "    dependent_Variable = str_DV_col\n",
    "\n",
    "    \n",
    "    X = df[independent_Variable]\n",
    "    if str_DV_col == \"ground_truth_activity\":\n",
    "        Y = df[dependent_Variable].replace(dict_independent_variable)\n",
    "    elif str_DV_col == \"ground_truth_time\":\n",
    "        Y = df[dependent_Variable]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def get_Regression_model_and_scaler(X, Y, type):\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    if type == \"Logistic\":\n",
    "        model = LogisticRegression()\n",
    "        Y = Y.fillna(0.0)\n",
    "    elif type == \"Linear\":\n",
    "        model = LinearRegression()\n",
    "        Y = Y.fillna(0.0)\n",
    "\n",
    "\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    intercept = model.intercept_\n",
    "\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isHolliday helper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isHoliday(timestamp):\n",
    "    if timestamp.weekday() in [5, 6] or timestamp.date() in holiday_list:\n",
    "        return 1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper method to get the needed dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IV_dict():\n",
    "    IV_list = get_all_Activities().tolist()\n",
    "    return {\n",
    "        a: IV_list.index(a) for a in IV_list\n",
    "    }\n",
    "\n",
    "def get_IV_dict_invert():\n",
    "    IV_dict = get_IV_dict()\n",
    "    return {v: k for k, v in IV_dict.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree helper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Decision_tree_model_and_scaler(X, Y, type):\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    if type == 'activity':\n",
    "        model = DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=1)\n",
    "    elif type == 'time':\n",
    "        model = DecisionTreeRegressor(criterion='absolute_error', max_depth=10, random_state=1)\n",
    "\n",
    "    Y = Y.fillna(0.0)\n",
    "\n",
    "    # print(Y)\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that deletes the outliers based on the zscore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_outliers(event_log):\n",
    "    traces = split_into_traces(event_log)\n",
    "    del_trace = []\n",
    "    trace_len = []\n",
    "    for j in range(1, len(traces)):\n",
    "        trace_len.append(traces[j].size)\n",
    "\n",
    "    df_len = pd.DataFrame(trace_len)\n",
    "    df_len = df_len[(np.abs(stats.zscore(df_len[0])) > 3)]\n",
    "\n",
    "    for ind in df_len.index:\n",
    "        del_trace.append(traces[ind][\"case concept:name\"][0])\n",
    "\n",
    "    for j in range(len(del_trace)):\n",
    "        event_log = event_log[event_log[\"case concept:name\"] != del_trace[j]]\n",
    "    event_log.to_csv(\"db/S2_Remove_Overlap.csv\")\n",
    "    return event_log.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation encoding\n",
    "\n",
    "- encode traces into numerical data for prediction\n",
    "- adds the ground truth at the start of the encoded prefix\n",
    "- adds the hollidays and last_event features at the end of the encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(events):\n",
    "\n",
    "    events_dict = {'ground_truth_activity' : 'truth', 'ground_truth_time' : 'truth'}\n",
    "    for event in events:\n",
    "        events_dict[event] = 0\n",
    "    events_dict['hollidays'] = 0\n",
    "    events_dict['last_event'] = 0\n",
    "    return events_dict\n",
    "\n",
    "def aggregation_encoding(prefixes, event_log):\n",
    "    events = get_all_Activities()\n",
    "    event_dict = create_dict(events)\n",
    "    last_event_dict = get_IV_dict()\n",
    "    aggregation_encoding = pd.DataFrame(event_dict, index=[0])\n",
    "    aggregation_encoding = aggregation_encoding.drop(0)\n",
    "    #new_encoding = ['truth', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    index = 0\n",
    "    for prefix in prefixes:\n",
    "        current_dict = create_dict(events)\n",
    "        current_dict['ground_truth_activity'] = prefix['ground_truth_activity'][len(prefix.index) - 1]\n",
    "        current_dict['ground_truth_time'] = prefix['ground_truth_time'][len(prefix.index) - 1]\n",
    "        current_dict['last_event'] = last_event_dict[prefix[\"event concept:name\"][len(prefix.index) - 1]]\n",
    "        for j in range (prefix.shape[0]):\n",
    "            if isHoliday(prefix[\"event time:timestamp\"][j]):\n",
    "                current_dict['hollidays'] = current_dict['hollidays'] + 1\n",
    "            current_dict[prefix[\"event concept:name\"][j]] = current_dict[prefix[\"event concept:name\"][j]] + 1\n",
    "        aggregation_encoding.loc[index] = current_dict.values()\n",
    "        index += 1\n",
    "    return aggregation_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the necesary changes to the event log and do all the steps in order to aggregate a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregation_encoding(path, type):\n",
    "    event_log = format_event_log(path)\n",
    "    if type == \"train\":\n",
    "        time_split = pd.to_datetime(\"02-01-2012 15:28:39.244\",format=\"%d-%m-%Y %H:%M:%S.%f\")\n",
    "        event_log = delete_overlapping_traces(event_log, pd.Timestamp(time_split, tz='GMT+0'))\n",
    "        event_log = delete_outliers(event_log)\n",
    "        event_log = format_event_log(\"db/S2_Remove_Overlap.csv\")\n",
    "    traces = split_into_traces(event_log)\n",
    "    prefix_lengths = []\n",
    "    if type == \"train\":\n",
    "        prefix_lengths = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29]\n",
    "    if type == \"test\":\n",
    "        prefix_lengths = [x for x in range(1, find_longest_trace(traces) + 1)]\n",
    "    prefixes = prefix_extraction(traces, prefix_lengths)\n",
    "    aggregation = aggregation_encoding(prefixes, event_log)\n",
    "    if type == \"train\":\n",
    "        aggregation.to_csv(\"db/aggregation_encoding_train.csv\")\n",
    "    if type == \"test\":\n",
    "        aggregation.to_csv(\"db/aggregation_encoding_test.csv\")\n",
    "    return aggregation\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the aggregations for the train and the test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_aggregation():\n",
    "    train_file_path = 'db/S1_GroundTruth.csv'\n",
    "    test_file_path = 'db/S2_Aggregation_Estimator.csv'\n",
    "    train_aggregation = get_aggregation_encoding(train_file_path, \"train\")\n",
    "    test_aggregation = get_aggregation_encoding(test_file_path, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Estimator (S1)\n",
    "- used indications on canvas\n",
    "- order train and test set by \"case concept:name\" and \"event time:timestamp\" columns \n",
    "- split train set into traces\n",
    "- find the length of the longest trace (N)\n",
    "- for each position i (from 1..N) find out the most commun activity that happened in each trace at position i at the most commun time difference between position i and position i-1 in each trace\n",
    "- add this data to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_estimators():\n",
    "    df_train = format_event_log(training_file_path)\n",
    "    df_train = writeGroundtruth(df_train)\n",
    "\n",
    "    df_test = format_event_log(testing_file_path)\n",
    "    df_test = writeGroundtruth(df_test)\n",
    "\n",
    "    traces = split_into_traces(df_train)\n",
    "    N = find_longest_trace(traces)\n",
    "    \n",
    "    common_activities = []\n",
    "    common_times = []\n",
    "    for i in range (0, N):\n",
    "        activity_at_position_i = defaultdict(int)\n",
    "        for trace in traces:\n",
    "            if (i < trace.shape[0]):\n",
    "                activity_at_position_i[trace[\"event concept:name\"][i]] += 1\n",
    "        common_activities.append(max(activity_at_position_i, key=lambda k: activity_at_position_i[k]))\n",
    "\n",
    "        time_at_position_i = defaultdict(int)\n",
    "        for trace in traces:\n",
    "            if (i < trace.shape[0]):\n",
    "                time_at_position_i[trace[\"ground_truth_time\"][i]] += 1\n",
    "                \n",
    "        common_times.append(max(time_at_position_i, key=lambda k: time_at_position_i[k]))\n",
    "\n",
    "    activity_predictions = []\n",
    "    time_predictions = []\n",
    "    index = 0\n",
    "    for j in range (0, df_test.shape[0] - 1):\n",
    "        if df_test[\"case concept:name\"][j] == df_test[\"case concept:name\"][j + 1]:\n",
    "            activity_predictions.append(common_activities[index])\n",
    "            time_predictions.append(common_times[index])\n",
    "            index += 1\n",
    "        else:\n",
    "            activity_predictions.append(common_activities[index])\n",
    "            time_predictions.append(common_times[index])\n",
    "            index = 0\n",
    "    activity_predictions.append(common_activities[index])\n",
    "    time_predictions.append(common_times[index])\n",
    "    df_test[\"S_1_Activity\"] = activity_predictions\n",
    "    df_test[\"S_1_Time\"] = time_predictions\n",
    "    df_train.to_csv(\"db/S1_GroundTruth.csv\")\n",
    "    df_test.to_csv(\"db/S1_Naive_estimator.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mode Estimator (S2)\n",
    "1. for each row find next activity of the case and its timestamp\n",
    "2. compute the time it take for the next event to be log in the db\n",
    "3. for each activity find the most common next activity (mode)\n",
    "4. for each activity find the average time between next activity\n",
    "5. Have 3. and 4. in a DataFrame\n",
    "6. base on the current activity write the prediction of the next activity and time it will take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_estimators():\n",
    "\n",
    "    s2_train_file_path = \"db/S1_GroundTruth.csv\"\n",
    "    s2_test_file_path = \"db/S1_Naive_estimator.csv\"\n",
    "\n",
    "    df_train = format_event_log(s2_train_file_path)\n",
    "    df_test = format_event_log(s2_test_file_path)\n",
    "\n",
    "    holiday_train = []\n",
    "    for index in range (df_train.shape[0]): \n",
    "        if isHoliday(df_train[\"event time:timestamp\"][index]):\n",
    "            holiday_train.append(1)\n",
    "        else:\n",
    "            holiday_train.append(0)\n",
    "\n",
    "    holiday_test = []\n",
    "    for index in range (df_test.shape[0]): \n",
    "        if isHoliday(df_test[\"event time:timestamp\"][index]):\n",
    "            holiday_test.append(1)\n",
    "        else:\n",
    "            holiday_test.append(0)\n",
    "\n",
    "    df_train['isHoliday'] = holiday_train\n",
    "    df_test['isHoliday'] = holiday_test\n",
    "\n",
    "    df_look_up_dict_activity =  df_train.groupby(['event concept:name', 'isHoliday']).agg(\n",
    "        Activity_Pretiction = ('ground_truth_activity', pd.Series.mode)\n",
    "    ).unstack()\n",
    "    df_look_up_dict_time = df_train.copy().groupby(['event concept:name', 'isHoliday']).agg(\n",
    "        Time_Pretiction = ('ground_truth_time', 'mean')\n",
    "    ).unstack()\n",
    "\n",
    "\n",
    "    df_test = df_test.assign(S_2_Activity='')\n",
    "    df_test = df_test.assign(S_2_Time=0)\n",
    "    for i, r in df_test.iterrows():\n",
    "        this_activity = r['event concept:name']\n",
    "        this_isHoliday = r['isHoliday']\n",
    "        next_event = df_look_up_dict_activity.loc[this_activity, ('Activity_Pretiction', this_isHoliday)]\n",
    "        next_time = df_look_up_dict_time.loc[this_activity, ('Time_Pretiction', this_isHoliday)]\n",
    "\n",
    "        df_test.at[i,'S_2_Activity'] = next_event\n",
    "        df_test.at[i,'S_2_Time'] = next_time\n",
    "\n",
    "    df_test = df_test.drop('isHoliday', axis = 1)\n",
    "    df_test.to_csv('db/S2_Aggregation_Estimator.csv')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivalue Regression (S3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Set up the training and testing aggregation encoding in two df\n",
    "2. Seet up the Dictionaray for converting activities to integer\n",
    "3. process the X Y value for both training and testing set\n",
    "4. call the '''get_Regression_model_and_scaler()''' with training values to get the model and the saler\n",
    "5. use the model to predict the activites in the testing set\n",
    "6. convert the prediction from integer back to activities(str) using the dictionary from step 2\n",
    "7. repeat step from 3. to 5. for time prediction\n",
    "\n",
    "For the activity prediction we use LogisticRegression() in the sklearn libeary, whereas we use LinearRegression() for the time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_estimator():\n",
    "    df_encoded_train = pd.read_csv('db/aggregation_encoding_train.csv')\n",
    "    df_encoded_test = pd.read_csv('db/aggregation_encoding_test.csv')\n",
    "    \n",
    "    df_output = format_event_log('db/S2_Aggregation_Estimator.csv')\n",
    "    \n",
    "    independent_Variable = get_all_Activities().tolist()\n",
    "    dict_IV_index_activities = {\n",
    "        a: independent_Variable.index(a) for a in independent_Variable\n",
    "    }\n",
    "    dict_IV_invert = {v: k for k, v in dict_IV_index_activities.items()}\n",
    "    # ========================================================================================== \n",
    "    X_train_activity, Y_train_activity = process_X_Y(df_encoded_train, \"ground_truth_activity\", dict_IV_index_activities)\n",
    "    X_test_activity, Y_test_activity = process_X_Y(df_encoded_test, \"ground_truth_activity\", dict_IV_index_activities)\n",
    "\n",
    "    model_activity, scaler_activity = get_Regression_model_and_scaler(\n",
    "        X_train_activity, \n",
    "        Y_train_activity, \n",
    "        \"Logistic\"\n",
    "    )\n",
    "\n",
    "    X_test_activity = scaler_activity.transform(X_test_activity)\n",
    "\n",
    "    predicts_activity = model_activity.predict(X_test_activity)\n",
    "    predicts_activity = [dict_IV_invert[p] for p in predicts_activity]\n",
    "\n",
    "    # ===========================================================================================\n",
    "    X_train_time, Y_train_time = process_X_Y(df_encoded_train, \"ground_truth_time\", dict_IV_index_activities)\n",
    "    X_test_time, Y_test_time = process_X_Y(df_encoded_test, \"ground_truth_time\", dict_IV_index_activities)\n",
    "    model_time, scaler_time = get_Regression_model_and_scaler(\n",
    "        X_train_time,\n",
    "        Y_train_time,\n",
    "        \"Linear\"\n",
    "    )\n",
    "    X_test_time = scaler_time.transform(X_test_time)\n",
    "    predicts_time = model_time.predict(X_test_time)\n",
    "\n",
    "    df_output['S_3_activities'] = predicts_activity\n",
    "    df_output['S_3_time'] = predicts_time\n",
    "    df_output.to_csv(\"db/S3_Regression_Estimator.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Estimator (S4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step to get this estimator is almost exactly the same as regression estimator, except we are using DecisionTreeClassifier() for activity prediction and DecisionTreeRegressor() for time prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_estimator():\n",
    "    df_encoded_train = pd.read_csv('db/aggregation_encoding_train.csv')\n",
    "    df_encoded_test = pd.read_csv('db/aggregation_encoding_test.csv')\n",
    "    \n",
    "    df_output = format_event_log('db\\S3_Regression_Estimator.csv')\n",
    "    \n",
    "    independent_Variable = get_all_Activities().tolist()\n",
    "    dict_IV_index_activities = get_IV_dict()\n",
    "    dict_IV_invert = get_IV_dict_invert()\n",
    "    # ========================================================================================== \n",
    "    X_train_activity, Y_train_activity = process_X_Y(df_encoded_train, \"ground_truth_activity\", dict_IV_index_activities)\n",
    "    X_test_activity, Y_test_activity = process_X_Y(df_encoded_test, \"ground_truth_activity\", dict_IV_index_activities)\n",
    "\n",
    "    model_activity, scaler_activity = get_Decision_tree_model_and_scaler(\n",
    "        X_train_activity, \n",
    "        Y_train_activity,\n",
    "        'activity'\n",
    "    )\n",
    "\n",
    "    X_test_activity = scaler_activity.transform(X_test_activity)\n",
    "\n",
    "    predicts_activity = model_activity.predict(X_test_activity)\n",
    "    predicts_activity = [dict_IV_invert[p] for p in predicts_activity]\n",
    "    \n",
    "    df_output['S_4_activities'] = predicts_activity\n",
    "\n",
    "\n",
    "    # ========================================================================================== \n",
    "    X_train_time, Y_train_time = process_X_Y(df_encoded_train, \"ground_truth_time\", dict_IV_index_activities)\n",
    "    X_test_time, Y_test_time = process_X_Y(df_encoded_test, \"ground_truth_time\", dict_IV_index_activities)\n",
    "\n",
    "    model_time, scaler_time = get_Decision_tree_model_and_scaler(\n",
    "        X_train_time, \n",
    "        Y_train_time,\n",
    "        'time'\n",
    "    )\n",
    "\n",
    "    X_test_time = scaler_activity.transform(X_test_time)\n",
    "\n",
    "    predicts_time = model_activity.predict(X_test_time)\n",
    "\n",
    "    df_output['S_4_time'] = predicts_time\n",
    "\n",
    "    df_output.to_csv(\"db/S4_Decision_Tree_Estimator.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplot representation of the entire data set in chronological order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_data():\n",
    "    df_BPI_train = pd.read_csv(training_file_path)\n",
    "    df_BPI_test = pd.read_csv(testing_file_path)\n",
    "    df_BPI = pd.concat([df_BPI_train, df_BPI_train])\n",
    "    \n",
    "    df_BPI['event time:timestamp'] = pd.to_datetime(df_BPI['event time:timestamp'],format=\"%d-%m-%Y %H:%M:%S.%f\")\n",
    "    \n",
    "    ax = sns.relplot(data=df_BPI, x='event time:timestamp', y='case concept:name', hue='event concept:name', height=10, aspect=1.5)\n",
    "    plt.ylim(max(df_BPI['case concept:name']), min(df_BPI['case concept:name']))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplot representation of the data set split into training and test sets, after the removal of unusable traces from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_split_data():\n",
    "    df_BPI_train = pd.read_csv(training_file_path)\n",
    "    df_BPI_test = pd.read_csv(testing_file_path)\n",
    "    df_BPI = pd.concat([df_BPI_train, df_BPI_train])\n",
    "    \n",
    "    df_BPI['event time:timestamp'] = pd.to_datetime(df_BPI['event time:timestamp'],format=\"%d-%m-%Y %H:%M:%S.%f\")\n",
    "    df_BPI['case REG_DATE'] = pd.to_datetime(df_BPI['case REG_DATE'],format=\"%d-%m-%Y %H:%M:%S.%f\")\n",
    "    splittime = pd.to_datetime(\"02-01-2012 15:28:39.244\",format=\"%d-%m-%Y %H:%M:%S.%f\")\n",
    "    plt_splitline = df_BPI.copy()\n",
    "    plt_splitline['event time:timestamp'] = splittime\n",
    "    plt_splitline['event concept:name'] = \"test/training split\"\n",
    "    \n",
    "    df_BPI = df_BPI.sort_values(by=['case concept:name','event time:timestamp'], ignore_index=True)\n",
    "    df_BPI_train = delete_overlapping_traces(df_BPI.copy(), splittime)\n",
    "    df_BPI_test = df_BPI[df_BPI['case REG_DATE'] >= splittime]\n",
    "    \n",
    "    df_BPI = pd.concat([df_BPI_train, df_BPI_test])\n",
    "    df_BPI = df_BPI.sort_values(by='event time:timestamp', ignore_index=True)\n",
    "    df_BPI = pd.concat([df_BPI, plt_splitline])\n",
    "    \n",
    "    ax = sns.relplot(data=df_BPI, x='event time:timestamp', y='case concept:name', hue='event concept:name', height=10, aspect=1.5)\n",
    "    plt.ylim(max(df_BPI['case concept:name']), min(df_BPI['case concept:name']))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplot representation of the data set with time arrangement of events relative to the start of their respective case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_case_relative():\n",
    "    df_BPI_train = pd.read_csv(training_file_path)\n",
    "    df_BPI_test = pd.read_csv(testing_file_path)\n",
    "    df_BPI = pd.concat([df_BPI_train, df_BPI_train])\n",
    "    \n",
    "    df_BPI['event time:timestamp'] = pd.to_datetime(df_BPI['event time:timestamp'],format=\"%d-%m-%Y %H:%M:%S.%f\")\n",
    "    df_BPI['case REG_DATE'] = pd.to_datetime(df_BPI['case REG_DATE'],format=\"%d-%m-%Y %H:%M:%S.%f\")\n",
    "    \n",
    "    df_BPI['event time:case relative'] = pd.to_datetime(df_BPI['event time:timestamp'], format=\"%Y-%m-%d %H:%M:%S.%f\") - pd.to_datetime(df_BPI['case REG_DATE'], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    \n",
    "    ax = sns.relplot(data=df_BPI, x='event time:case relative', y='case concept:name', hue='event concept:name', markers=\"x\", height=7, aspect=1.5)\n",
    "    plt.ylim(max(df_BPI['case concept:name']), min(df_BPI['case concept:name']))\n",
    "    plt.xlim(0, )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter Plot representation of the time error of the currently checked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_visualisation_time(df, prediction_time):\n",
    "    df['ground_truth_time'] = df['ground_truth_time'] / 3600\n",
    "    df[prediction_time] = df[prediction_time] / 3600\n",
    "    \n",
    "    x1 = df['ground_truth_time']\n",
    " \n",
    "    y1 = df.index\n",
    " \n",
    "    x2 = df[prediction_time]\n",
    " \n",
    "    y2 = df.index\n",
    " \n",
    "    plt.scatter(x1, y1, c =\"green\",\n",
    "                linewidths = 2,\n",
    "                marker =\"s\",\n",
    "                s = 10)\n",
    "    \n",
    "    plt.scatter(x2, y2, c =\"red\",\n",
    "                linewidths = 2,\n",
    "                marker =\"^\",\n",
    "                s = 10)\n",
    "    \n",
    "    plt.xlabel(\"Time (Hours)\")\n",
    "    plt.ylabel(\"EventID\")\n",
    "    plt.ylim(200, 300)\n",
    "    plt.xlim(-2, 200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter Plot representation of the time error of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_visualisation_time_all():\n",
    "    df_error_model_1 = pd.read_csv('db/S1_Naive_estimator.csv')\n",
    "    df_error_model_1 = df_error_model_1.fillna('None')\n",
    "    \n",
    "    df_error_model_2 = pd.read_csv('db/S2_Aggregation_Estimator.csv')\n",
    "    df_error_model_2 = df_error_model_2.fillna('None')\n",
    "    \n",
    "    df_error_model_3 = pd.read_csv('db/S3_Regression_Estimator.csv')\n",
    "    df_error_model_3 = df_error_model_3.fillna('None')\n",
    "    \n",
    "    df_error_model_4 = pd.read_csv('db/S4_Decision_Tree_Estimator.csv')\n",
    "    df_error_model_4 = df_error_model_4.fillna('None')\n",
    "    \n",
    "    df_error_model_1['ground_truth_time'] = df_error_model_1['ground_truth_time'] / 3600\n",
    "    df_error_model_1['S_1_time'] = df_error_model_2['S_1_Time'] / 3600\n",
    "    df_error_model_2['S_2_time'] = df_error_model_2['S_2_Time'] / 3600\n",
    "    df_error_model_3['S_3_time'] = df_error_model_3['S_3_time'] / 3600\n",
    "    df_error_model_4['S_4_time'] = df_error_model_4['S_4_time'] / 3600\n",
    "    \n",
    "    x1 = df_error_model_2['ground_truth_time']\n",
    "    y1 = df_error_model_1.index\n",
    "    \n",
    "    x5 = df_error_model_1['S_1_Time']\n",
    " \n",
    "    x2 = df_error_model_2['S_2_Time']\n",
    "    \n",
    "    x3 = df_error_model_3['S_3_time']\n",
    "    \n",
    "    x4 = df_error_model_4['S_4_time']\n",
    " \n",
    "    plt.scatter(x1, y1, c =\"green\",\n",
    "                linewidths = 2,\n",
    "                marker =\"s\",\n",
    "                s = 10)\n",
    "    \n",
    "    plt.scatter(x5, y1, c =\"blue\",\n",
    "                linewidths = 2,\n",
    "                marker =\"^\",\n",
    "                s = 10)\n",
    "    \n",
    "    plt.scatter(x2, y1, c =\"red\",\n",
    "                linewidths = 2,\n",
    "                marker =\"^\",\n",
    "                s = 10)\n",
    "    \n",
    "    plt.scatter(x3, y1, c =\"purple\",\n",
    "                linewidths = 2,\n",
    "                marker =\"^\",\n",
    "                s = 10)\n",
    "    \n",
    "#     plt.scatter(x4, y1, c =\"orange\",\n",
    "#                 linewidths = 2,\n",
    "#                 marker =\".\",\n",
    "#                 s = 10)\n",
    "    \n",
    "    plt.xlabel(\"Time (Hours)\")\n",
    "    plt.ylabel(\"EventID\")\n",
    "    plt.ylim(200, 300)\n",
    "    plt.xlim(-2, 200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap Representing a Confusion Matrix for our activity predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_visualisation_activity(prediction_path, df, prediction_activity):\n",
    "    cm = confusion_matrix(df['ground_truth_activity'], df[prediction_activity])\n",
    "    plt.figure(dpi =1200)\n",
    "    labels = np.array(['A_ACCEPTED','A_ACTIVATED','A_APPROVED', 'A_CANCELLED','A_DECLINED','A_FINALIZED',\n",
    "                       'A_PARTLYSUBMITTED','A_PREACCEPTED','A_REGISTERED','NaN','A_SUBMITTED','O_ACCEPTED',\n",
    "                       'O_CANCELLED','O_CREATED','O_DECLINED','O_SELECTED','O_SENT','O_SENT_BACK',\n",
    "                       'W_Afhandelen leads','W_Beoordelen fraude','W_Completeren aanvraag',\n",
    "                       'W_nabellen incomplete dossiers','W_nabellen offertes','W_valideren aanvraag',])\n",
    "    sns.heatmap(cm,\n",
    "            annot=True, \n",
    "            annot_kws={\"size\":5},\n",
    "            fmt='g',\n",
    "            xticklabels=labels,\n",
    "            yticklabels=labels)\n",
    "    plt.ylabel('Actual',fontsize=7)\n",
    "    plt.xlabel('Predicted',fontsize=7)\n",
    "    plt.title(prediction_path,fontsize=10)\n",
    "    plt.tick_params(labelsize=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds the Accuracy of the activity prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_activity(df, prediction_activity):\n",
    "    df['error_activity'] = df['ground_truth_activity'] == df[prediction_activity]\n",
    "    correct_predictions = df['error_activity'].value_counts()\n",
    "    percentage = correct_predictions[True] / len(df['error_activity']) * 100\n",
    "    print(f'Percentage of True values: {percentage:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the absolute mean error of the time prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_time(df, prediction_time):\n",
    "    df['error_time'] = df[prediction_time] - df['ground_truth_time']\n",
    "    mean_absolute_error = df['error_time'].mean()\n",
    "    print(abs(mean_absolute_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs all the methods to see error of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_model(predictor_path, prediction_activity, prediction_time):\n",
    "    df_error_model = pd.read_csv(predictor_path)\n",
    "    df_error_model = df_error_model.fillna('None')\n",
    "    error_activity(df_error_model, prediction_activity)\n",
    "    error_time(df_error_model, prediction_time)\n",
    "    error_visualisation_time(df_error_model, prediction_time)\n",
    "    error_visualisation_activity(predictor_path, df_error_model, prediction_activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main class\n",
    "1. All functions that are needed all called in this cell.\n",
    "2. No other cell runs code other than the main cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# naive_estimators()\n",
    "# mode_estimators()\n",
    "# write_aggregation()\n",
    "# regression_estimator()\n",
    "# decision_tree_estimator()\n",
    "# error_model('db/S3_Regression_Estimator.csv', 'S_3_activities', 'S_3_time')\n",
    "# error_model('db/S2_Aggregation_Estimator.csv', 'S_2_Activity', 'S_2_Time')\n",
    "error_visualisation_time_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c57489fe19794b56e8aa35d040b5ad0fe5f8282e54818702f4c4eecc6496688"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
