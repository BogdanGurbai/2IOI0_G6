{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py as pm\n",
    "import numpy as np\n",
    "import psutil as pu\n",
    "\n",
    "from  sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score  \n",
    "\n",
    "training_file_path = 'db/BPI_Challenge_2012.XE-training.csv'\n",
    "testing_file_path = 'db/BPI_Challenge_2012.XE-test.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load CSV with pandas and pm4py\n",
    "\n",
    "pm4py gives two extra columns: @@index and @@case_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_event_log(path):\n",
    "    event_log = pd.read_csv(path, sep=',')\n",
    "    event_log = pm.format_dataframe(\n",
    "        event_log, \n",
    "        case_id='case concept:name', \n",
    "        activity_key='event concept:name', \n",
    "        timestamp_key='event time:timestamp'\n",
    "    )\n",
    "    return event_log\n",
    "#event_log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genernal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that finds the length of the longest trace in a list of traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_trace(traces):\n",
    "    max = 0\n",
    "    for trace in traces:\n",
    "        if trace.shape[0] > max:\n",
    "            max = trace.shape[0]\n",
    "    return max\n",
    "\n",
    "def get_all_Activities():\n",
    "    df = format_event_log(training_file_path)\n",
    "    arr = df['event concept:name'].unique()\n",
    "    return arr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that adds the Ground truth collumns for event and time of the event\n",
    "- Every value in the ground truth collumns is the actual next event or time of the next evet in the data set\n",
    "- Used for naive simulator\n",
    "- Used for verification purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume sorted by caseID and time\n",
    "def caseHasNextEvent(df, index):\n",
    "    if index >= len(df) - 1:\n",
    "        return False\n",
    "    if df.loc[index, 'case concept:name'] == df.loc[index+1, 'case concept:name']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def writeGroundtruth(df):\n",
    "    df = df.sort_values(by=['case concept:name','event time:timestamp'])\n",
    "    #add new columns containing the name of the next event in the case and the time when it happens\n",
    "    df = df.assign(ground_truth_activity='')\n",
    "    df = df.assign(ground_truth_time='')\n",
    "\n",
    "    for ind in df.index:\n",
    "        if caseHasNextEvent(df, ind):\n",
    "            df.at[ind,'ground_truth_activity'] = df.loc[ind+1,'event concept:name']\n",
    "            df.at[ind,'ground_truth_time'] = df.loc[ind+1, 'event time:timestamp']\n",
    "        else:\n",
    "            df.at[ind,'ground_truth_activity'] = None\n",
    "            df.at[ind,'ground_truth_time'] = None\n",
    "    return df\n",
    "\n",
    "#df_event = writeGroundtruth(event_log)\n",
    "#df_event.to_csv('check_writeGroundtruth_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTimeDifference(df):\n",
    "    # df['time_until_next_event'] = 0  # initialize new column with zeros\n",
    "    df = df.assign(time_until_next_event=0) # initialize new column\n",
    "\n",
    "    # iterate over each row of the dataframe\n",
    "    for i, row in df.iterrows():        \n",
    "        # check if there is a next row with the same case\n",
    "        if caseHasNextEvent(df, i):\n",
    "            nextTime = df.loc[i+1, 'event time:timestamp']\n",
    "            currentTime = row['event time:timestamp']\n",
    "            timeDiff = nextTime - currentTime\n",
    "            df.at[i, 'time_until_next_event'] = timeDiff.total_seconds()\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that splits the data set into separate traces\n",
    "- Parameter $traces$ is a list containing all the individual traces\n",
    "- Each trace in traces is a list containing all the events in the trace in the order that they happen\n",
    "- Helper method for prefix extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_traces(event_log):\n",
    "    traces = []\n",
    "    last_location = 0\n",
    "    for j in range (0, event_log.shape[0] - 1):\n",
    "        if not event_log[\"case concept:name\"][j] == event_log[\"case concept:name\"][j + 1]:\n",
    "            traces.append(event_log.loc[last_location : j].reset_index())\n",
    "            last_location = j + 1\n",
    "    traces.append(event_log.loc[last_location : event_log.shape[0] - 1].reset_index())\n",
    "    return traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that deletes traces from a list that contain events past a specified time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_overlaping_traces(traces, end_time):\n",
    "    del_tr = []\n",
    "    for j in range(len(traces)):\n",
    "        for k in traces[j].index:\n",
    "            trace_ts = traces[j][\"event time:timestamp\"][k]\n",
    "        if trace_ts > end_time:\n",
    "            del_tr.append(j)\n",
    "\n",
    "    i = 0\n",
    "    for j in range(len(del_tr)):\n",
    "        del traces[del_tr[j] - i]\n",
    "        i = i + 1\n",
    "    return traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefix extraction\n",
    "- extract event lists of odd length like 1, 3, 5, 7, 9, 11, 13, 15 from the traces for prediction\n",
    "- and the ground truth\n",
    "- store them in a list of prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_extraction(traces, prefix_lengths):\n",
    "    prefixes = []\n",
    "    for trace in traces:\n",
    "        for length in prefix_lengths:\n",
    "            if trace.shape[0] >= length:\n",
    "                prefixes.append(trace.loc[:length - 1])\n",
    "    return prefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregation encoding\n",
    "- endoce traces into numerical data for prediction\n",
    "- adds the ground truth at the end of the encoded prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(event_log):\n",
    "    events = []\n",
    "    events = event_log['event concept:name'].unique()\n",
    "\n",
    "    events_dict = {'ground_truth' : 'truth'}\n",
    "    for event in events:\n",
    "        events_dict[event] = 0\n",
    "    return events_dict\n",
    "\n",
    "def aggregation_encoding(prefixes, event_log):\n",
    "    event_dict = create_dict(event_log)\n",
    "    aggregation_encoding = pd.DataFrame(event_dict, index=[0])\n",
    "    aggregation_encoding = aggregation_encoding.drop(0)\n",
    "    #new_encoding = ['truth', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    index = 0\n",
    "    for prefix in prefixes:\n",
    "        current_dict = create_dict(event_log)\n",
    "        current_dict['ground_truth'] = prefix['ground_truth_activity'][len(prefix.index) - 1]\n",
    "        for event in prefix[\"event concept:name\"]:\n",
    "            current_dict[event] = current_dict[event] + 1\n",
    "        aggregation_encoding.loc[index] = current_dict.values()\n",
    "        index += 1\n",
    "    return aggregation_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregation_encoding(path, type):\n",
    "    event_log = format_event_log(path)\n",
    "    traces = split_into_traces(event_log)\n",
    "    prefix_lengths = []\n",
    "    if type == \"train\":\n",
    "        #traces = delete_overlaping_traces(traces, pd.to_datetime(\"02-01-2012 15:28:39.244\",format=\"%d-%m-%Y %H:%M:%S.%f\"))\n",
    "        prefix_lengths = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29]\n",
    "    if type == \"test\":\n",
    "        prefix_lengths = [x for x in range(1, find_longest_trace(traces) + 1)]\n",
    "    prefixes = prefix_extraction(traces, prefix_lengths)\n",
    "    aggregation = aggregation_encoding(prefixes, event_log)\n",
    "    if type == \"train\":\n",
    "        aggregation.to_csv(\"db/aggregation_encoding_train.csv\")\n",
    "    if type == \"test\":\n",
    "        aggregation.to_csv(\"db/aggregation_encoding_test.csv\")\n",
    "    return aggregation\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_aggregation():\n",
    "    train_file_path = 'db/Ground_Turth.csv'\n",
    "    test_file_path = 'db/Naive_Estimator.csv'\n",
    "    train_aggregation = get_aggregation_encoding(train_file_path, \"train\")\n",
    "    test_aggregation = get_aggregation_encoding(test_file_path, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_X_Y(df, dependent_Variable):\n",
    "    # dependent_Variable = 'ground_truth'\n",
    "    independent_Variable = get_all_Activities().tolist()\n",
    "\n",
    "    dict_activity_to_int = {a: independent_Variable.index(a) for a in independent_Variable}\n",
    "    \n",
    "    X = df[independent_Variable]\n",
    "    Y = df[dependent_Variable].replace(dict_activity_to_int)\n",
    "\n",
    "    print(independent_Variable)\n",
    "    return X, Y, dict_activity_to_int\n",
    "\n",
    "def get_Regression_model_scaler(df_training_encoded):\n",
    "    X, Y, _ = process_X_Y(df_training_encoded)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    intercept = model.intercept_\n",
    "\n",
    "    return model, scaler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Estimator (S1)\n",
    "1. for each row find next activity of the case and its timestamp\n",
    "2. compute the time it take for the next event to be log in the db\n",
    "3. for each activity find the most common next activity (mode)\n",
    "4. for each activity find the average time between next activity\n",
    "5. Have 3. and 4. in a DataFrame\n",
    "6. base on the current activity write the prediction of the next activity and time it will take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_estimators():\n",
    "    df_train = format_event_log(training_file_path)\n",
    "    df_train = writeGroundtruth(df_train)\n",
    "    df_train = computeTimeDifference(df_train)\n",
    "\n",
    "    df_test = format_event_log(testing_file_path)\n",
    "    df_test = writeGroundtruth(df_test)\n",
    "\n",
    "\n",
    "    df_naive_predictor_dict = df_train.groupby(['event concept:name']).agg(\n",
    "        naive_prediction_activity = ('ground_truth_activity', pd.Series.mode),\n",
    "        naive_prediction_time = ('time_until_next_event', 'mean')\n",
    "    )\n",
    "\n",
    "    df_test = df_test.assign(naive_prediction_activity='')\n",
    "    df_test = df_test.assign(naive_prediction_time=0)\n",
    "    for i, r in df_test.iterrows():\n",
    "        this_event = r['event concept:name']\n",
    "        next_event = df_naive_predictor_dict.loc[this_event,'naive_prediction_activity']\n",
    "        next_event_time = df_naive_predictor_dict.loc[this_event,'naive_prediction_time']\n",
    "        df_test.at[i,'naive_prediction_activity'] = next_event\n",
    "        df_test.at[i,'naive_prediction_time'] = next_event_time\n",
    "    df_train.to_csv('db/Ground_Turth.csv')\n",
    "    df_test.to_csv('db/Navie_Estimator.csv')\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivalue Regression (S2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_estimator():\n",
    "    df_encoded_train = pd.read_csv('db/aggregation_encoding.csv')\n",
    "    df_encoded_test = pd.read_csv('db/aggregation_encoding_test.csv')\n",
    "    \n",
    "    df_output = format_event_log('db/Naive_Estimator.csv')\n",
    "    \n",
    "    model, scaler = get_Regression_model_scaler(df_encoded_train)\n",
    "    X_test, Y_test, dict_activities_int = process_X_Y(df_encoded_test)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    dict_int_activities = {v: k for k, v in dict_activities_int.items()}\n",
    "\n",
    "    predicts = model.predict(X_test)\n",
    "    predicts = [dict_int_activities[p] for p in predicts]\n",
    "\n",
    "    df_output['RE P activities'] = predicts\n",
    "    df_output.to_csv(\"db/Regression_Estimator.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_model_activity():\n",
    "    predictor_path = \"naive_predictor_result.csv\"\n",
    "    df_error_model = pd.read_csv(predictor_path)\n",
    "    df_error_model['error_activity'] = df_error_model['ground_truth_activity'] == df_error_model['naive_prediction_activity']\n",
    "    df_error_model['error_time'] = df_error_model['naive_prediction_time'] - df_error_model['time_until_next_event']\n",
    "    correct_predictions = df_error_model['error_activity'].value_counts()\n",
    "    percentage = correct_predictions[True] / len(df_error_model['error_activity']) * 100\n",
    "    mean_absolute_error = df_error_model['error_time'].mean()\n",
    "    print(f'Percentage of True values: {percentage:.1f}%')\n",
    "    print(mean_absolute_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory and CPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_memory_cpu() : \n",
    "    print('The CPU usage is: ', pu.cpu_percent(4))\n",
    "    print('RAM memory % used:', pu.virtual_memory()[2])\n",
    "    print('RAM Used (GB):', pu.virtual_memory()[3]/1000000000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main class\n",
    "1. All functions that are needed all called in this cell.\n",
    "2. No other cell runs code other than the main cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linyu\\AppData\\Local\\Temp\\ipykernel_18464\\945299891.py:3: DeprecatedWarning: format_dataframe is deprecated as of 2.3.0 and will be removed in 3.0.0. the format_dataframe function does not need application anymore.\n",
      "  event_log = pm.format_dataframe(\n",
      "C:\\Users\\linyu\\AppData\\Local\\Temp\\ipykernel_18464\\945299891.py:3: DeprecatedWarning: format_dataframe is deprecated as of 2.3.0 and will be removed in 3.0.0. the format_dataframe function does not need application anymore.\n",
      "  event_log = pm.format_dataframe(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A_SUBMITTED', 'A_PARTLYSUBMITTED', 'A_PREACCEPTED', 'W_Completeren aanvraag', 'A_ACCEPTED', 'A_FINALIZED', 'O_SELECTED', 'O_CREATED', 'O_SENT', 'W_Nabellen offertes', 'O_SENT_BACK', 'W_Valideren aanvraag', 'O_ACCEPTED', 'A_REGISTERED', 'A_APPROVED', 'A_ACTIVATED', 'O_CANCELLED', 'W_Wijzigen contractgegevens', 'A_DECLINED', 'A_CANCELLED', 'W_Afhandelen leads', 'O_DECLINED', 'W_Nabellen incomplete dossiers', 'W_Beoordelen fraude']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linyu\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\linyu\\AppData\\Local\\Temp\\ipykernel_18464\\945299891.py:3: DeprecatedWarning: format_dataframe is deprecated as of 2.3.0 and will be removed in 3.0.0. the format_dataframe function does not need application anymore.\n",
      "  event_log = pm.format_dataframe(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A_SUBMITTED', 'A_PARTLYSUBMITTED', 'A_PREACCEPTED', 'W_Completeren aanvraag', 'A_ACCEPTED', 'A_FINALIZED', 'O_SELECTED', 'O_CREATED', 'O_SENT', 'W_Nabellen offertes', 'O_SENT_BACK', 'W_Valideren aanvraag', 'O_ACCEPTED', 'A_REGISTERED', 'A_APPROVED', 'A_ACTIVATED', 'O_CANCELLED', 'W_Wijzigen contractgegevens', 'A_DECLINED', 'A_CANCELLED', 'W_Afhandelen leads', 'O_DECLINED', 'W_Nabellen incomplete dossiers', 'W_Beoordelen fraude']\n"
     ]
    }
   ],
   "source": [
    "regression_estimator()\n",
    "# get_all_Activities()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8b70c613820eb82f44088773ffc2add453462ad8308f78b60a549bce82e221db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
